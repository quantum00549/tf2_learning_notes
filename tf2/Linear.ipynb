{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for _ in range(100):\n",
    "    x = np.random.uniform(-10, 10)\n",
    "    eps = np.random.normal(0, 0.01)\n",
    "    data.append([x, 1.477 * x + 0.089 + eps])\n",
    "data = np.array(data)\n",
    "# 随机生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(b, w, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x, y = points[i]\n",
    "        total_error += (y - (w * x + b))**2\n",
    "    return total_error / float(len(points))\n",
    "# 平均均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_grad(b_current, w_current, points, lr):\n",
    "    b_grad = w_grad = 0\n",
    "    M = float(len(points))\n",
    "    for i in range(len(points)):\n",
    "        x, y = points[i]\n",
    "        b_grad += (2 / M) * (w_current * x + b_current - y)\n",
    "        w_grad += (2 / M) * x * (w_current * x + b_current - y)\n",
    "    b_new = b_current-lr * b_grad\n",
    "    w_new = w_current-lr * w_grad\n",
    "    return b_new, w_new\n",
    "# 更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(points, b_starting, w_starting, lr, num_iterations):\n",
    "    b = b_starting\n",
    "    w = w_starting\n",
    "    for step in range(num_iterations):\n",
    "        b, w = step_grad(b, w, points, lr)\n",
    "        loss = mse(b, w, points)\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f'iteration:{step+1},loss:{loss},w:{w},b:{b}')\n",
    "    return b,w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def gradient_descent(points, b_starting, w_starting, lr, num_iterations):\n",
    "    b = b_starting\n",
    "    w = w_starting\n",
    "    for step in range(num_iterations):\n",
    "        b_grad = w_grad = 0\n",
    "        M = float(len(points))\n",
    "        for i in range(len(points)):\n",
    "            x, y = points[i]\n",
    "            b_grad += (2 / M) * (w * x + b - y)\n",
    "            w_grad += (2 / M) * x * (w * x + b - y)\n",
    "        b -= lr * b_grad\n",
    "        w -= lr * w_grad\n",
    "        loss = mse(b, w, points)\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f'iteration:{step+1},loss:{loss},w:{w},b:{b}')\n",
    "    return b, w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lr=0.01,initial_b=0,initial_w=0,num_iterations=1000):\n",
    "    b,w=gradient_descent(data,initial_b,initial_w,lr,num_iterations)\n",
    "    loss=mse(b,w,data)\n",
    "    print(f'Final loss:{loss}, w:{w}, b:{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:50,loss:0.0008550566151263688,w:1.4773484360315916,b:0.062271532554694684\n",
      "iteration:100,loss:0.00018899080804948666,w:1.4771812464671072,b:0.07989385829600572\n",
      "iteration:150,loss:0.00010014635382475252,w:1.4771201852075384,b:0.0863299143358609\n",
      "iteration:200,loss:8.829566735710218e-05,w:1.4770978843095628,b:0.08868050176434383\n",
      "iteration:250,loss:8.67149410081136e-05,w:1.477089739537345,b:0.08953898733760907\n",
      "iteration:300,loss:8.650409281824503e-05,w:1.4770867648896626,b:0.0898525249114827\n",
      "iteration:350,loss:8.64759684315048e-05,w:1.4770856784837327,b:0.0899670356421135\n",
      "iteration:400,loss:8.647221700670005e-05,w:1.4770852817046953,b:0.09000885744760741\n",
      "iteration:450,loss:8.647171661577022e-05,w:1.4770851367923745,b:0.09002413167915883\n",
      "iteration:500,loss:8.64716498701736e-05,w:1.477085083867248,b:0.09002971016021036\n",
      "iteration:550,loss:8.647164096718514e-05,w:1.4770850645378426,b:0.09003174754263028\n",
      "iteration:600,loss:8.647163977964419e-05,w:1.477085057478324,b:0.0900324916389331\n",
      "iteration:650,loss:8.647163962124199e-05,w:1.4770850549000345,b:0.09003276339906124\n",
      "iteration:700,loss:8.647163960011325e-05,w:1.477085053958387,b:0.09003286265176154\n",
      "iteration:750,loss:8.64716395972917e-05,w:1.477085053614477,b:0.09003289890100305\n",
      "iteration:800,loss:8.647163959691824e-05,w:1.4770850534888735,b:0.09003291214001322\n",
      "iteration:850,loss:8.64716395968667e-05,w:1.4770850534430007,b:0.09003291697518719\n",
      "iteration:900,loss:8.64716395968622e-05,w:1.4770850534262467,b:0.09003291874109774\n",
      "iteration:950,loss:8.64716395968607e-05,w:1.4770850534201279,b:0.09003291938604664\n",
      "iteration:1000,loss:8.647163959685941e-05,w:1.4770850534178932,b:0.0900329196215959\n",
      "Final loss:8.647163959685941e-05, w:1.4770850534178932, b:0.0900329196215959\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 49, loss: 9.430634904909672e-05\n",
      "Iteration: 99, loss: 9.430634904979557e-05\n",
      "Iteration: 149, loss: 9.430634904979557e-05\n",
      "Iteration: 199, loss: 9.430634904979557e-05\n",
      "Iteration: 249, loss: 9.430634904979557e-05\n",
      "Iteration: 299, loss: 9.430634904979557e-05\n",
      "Iteration: 349, loss: 9.430634904979557e-05\n",
      "Iteration: 399, loss: 9.430634904979557e-05\n",
      "Iteration: 449, loss: 9.430634904979557e-05\n",
      "Iteration: 499, loss: 9.430634904979557e-05\n",
      "Iteration: 549, loss: 9.430634904979557e-05\n",
      "Iteration: 599, loss: 9.430634904979557e-05\n",
      "Iteration: 649, loss: 9.430634904979557e-05\n",
      "Iteration: 699, loss: 9.430634904979557e-05\n",
      "Iteration: 749, loss: 9.430634904979557e-05\n",
      "Iteration: 799, loss: 9.430634904979557e-05\n",
      "Iteration: 849, loss: 9.430634904979557e-05\n",
      "Iteration: 899, loss: 9.430634904979557e-05\n",
      "Iteration: 949, loss: 9.430634904979557e-05\n",
      "Iteration: 999, loss: 9.430634904979557e-05\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def gen_data():\n",
    "    data = []\n",
    "    for _ in range(100):\n",
    "        x = np.random.uniform(-10, 10)\n",
    "        eps = np.random.normal(0, 0.01)\n",
    "        data.append([x, 1.477 * x + 0.089 + eps])\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "class linealModel():\n",
    "    def __init__(self,lr,num_iteration,w=0,b=0):\n",
    "        self.lr = lr\n",
    "        self.num_iteration = num_iteration\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def __mse(self,data):\n",
    "        total_error = 0\n",
    "        for _ in range(len(data)):\n",
    "            x,y = data[_]\n",
    "            total_error += (y - self.w*x - self.b)**2\n",
    "        return total_error/len(data)\n",
    "    def __step_grad(self,data):\n",
    "        w_grad = b_grad =0\n",
    "        for _ in range(len(data)):\n",
    "            x,y = data[_]\n",
    "            w_grad += (2 / len(data)) * x * (self.w * x + self.b - y)\n",
    "            b_grad += (2 / len(data)) * (self.w * x + self.b - y)\n",
    "            self.w -= self.lr*w_grad\n",
    "            self.b -= self.lr*b_grad\n",
    "    def train(self,data):\n",
    "        for _ in range(self.num_iteration):\n",
    "            self.__step_grad(data)\n",
    "            if (_+1)%50 == 0:\n",
    "                loss = self.__mse(data)\n",
    "                print(f'Iteration: {_}, loss: {loss}')\n",
    "    def __forward(self,x):\n",
    "        return self.w*x + self.b\n",
    "    def __call__(self,x):\n",
    "        return self.__forward(x)\n",
    "    \n",
    "config = {\n",
    "    'lr':0.01,\n",
    "    'num_iteration':1000,\n",
    "    'w':0,\n",
    "    'b':0,\n",
    "}\n",
    "data = gen_data()\n",
    "model = linealModel(**config)\n",
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4755598416422044"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.08939470735065973"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w\n",
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
